{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from path import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from imageio import imread, imwrite\n",
    "import numpy as np\n",
    "from util import flow2rgb\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import kaiming_normal_, constant_\n",
    "\n",
    "from convlstm import ConvLSTM\n",
    "\n",
    "from deepVO_wMemory import DvoAm_EncPlusTrack, DvoAm_Encoder\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'flownets'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "FlowNet_data = torch.load('flownets_EPE1.951.pth',map_location=torch.device('cpu'))\n",
    "print(\"=> using pre-trained model '{}'\".format(FlowNet_data[\"arch\"]))\n",
    "FlowNetModel = models.__dict__[FlowNet_data[\"arch\"]](FlowNet_data).to(device)\n",
    "#print(model)\n",
    "#FlowNetModel.eval()\n",
    "#print(FlowNetModel.state_dict()['conv1.0.weight'].data.size())\n",
    "#print(FlowNetModel.state_dict()['conv1.0.weight'].data)\n",
    "#print(FlowNetModel.named_parameters())\n",
    "\n",
    "model2 = DvoAm_EncPlusTrack(batchNorm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoding.conv1.0.weight', 'encoding.conv1.0.bias', 'encoding.conv2.0.weight', 'encoding.conv2.0.bias', 'encoding.conv3.0.weight', 'encoding.conv3.0.bias', 'encoding.conv3_1.0.weight', 'encoding.conv3_1.0.bias', 'encoding.conv4.0.weight', 'encoding.conv4.0.bias', 'encoding.conv4_1.0.weight', 'encoding.conv4_1.0.bias', 'encoding.conv5.0.weight', 'encoding.conv5.0.bias', 'encoding.conv5_1.0.weight', 'encoding.conv5_1.0.bias', 'encoding.conv6.0.weight', 'encoding.conv6.0.bias', 'convLSTM.cell_list.0.conv.weight', 'convLSTM.cell_list.0.conv.bias', 'fc.weight', 'fc.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Capturing the layers we want from FlowNet!\n",
    "encodingLayers = ['conv1.0.weight', 'conv2.0.weight', \\\n",
    "                  'conv3.0.weight', 'conv3_1.0.weight', \\\n",
    "                  'conv4.0.weight', 'conv4_1.0.weight', \\\n",
    "                  'conv5.0.weight', 'conv5_1.0.weight', \\\n",
    "                  'conv6.0.weight',\\\n",
    "                  'conv1.0.bias', 'conv2.0.bias', \\\n",
    "                  'conv3.0.bias', 'conv3_1.0.bias', \\\n",
    "                  'conv4.0.bias', 'conv4_1.0.bias', \\\n",
    "                  'conv5.0.bias', 'conv5_1.0.bias', \\\n",
    "                  'conv6.0.bias' ]\n",
    "subset_state_dict = {}\n",
    "for name, param in FlowNetModel.named_parameters():\n",
    "    if name in encodingLayers:  # Change 'desired_layer' to the relevant layer names\n",
    "        subset_state_dict[name] = param\n",
    "#print(subset_state_dict)\n",
    "\n",
    "\n",
    "new_state_dict = model2.state_dict()\n",
    "for name, param in subset_state_dict.items():\n",
    "    #print(name)\n",
    "    newName = 'encoding.'+name\n",
    "    if newName in new_state_dict:\n",
    "        new_state_dict[newName].copy_(param) #copy_() performs copy in place\n",
    "print(new_state_dict.keys())\n",
    "model2.load_state_dict(new_state_dict)\n",
    "\n",
    "#model2.state_dict()['conv1.0.weight'] = FlowNetModel.state_dict()['conv1.0.weight']\n",
    "#print(model2.state_dict()['encoding.conv1.0.weight'])\n",
    "#print(model2.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction(yPred, yGt):\n",
    "    lossLocal = 1\n",
    "    lossGlobal = 2\n",
    "    loss = lossLocal +  lossGlobal\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decays every 60k iterations\n",
    "learningRateInitial = 0.0001\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.00001, betas=(0.9, 0.99), weight_decay=0.0004)  # Adjust the LR as needed\n",
    "\n",
    "numEpochs = 10\n",
    "batchSize = 4\n",
    "def train():\n",
    "    for epoch in range(numEpochs):\n",
    "        for i in range(0, len(X), batchSize):\n",
    "            batchX = 3#....\n",
    "            batchY = 3#....\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model2(batchX)\n",
    "            loss = lossFunction()\n",
    "            loss.backwards()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{numEpochs}], Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare datasets:\n",
    "train_images = []\n",
    "test_images = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
